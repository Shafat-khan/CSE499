{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lZYo0UOzr1TJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BeitForImageClassification, BeitFeatureExtractor\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, num_classes, use_pretrained=True, feature_extract=True):\n",
        "        super(HybridModel, self).__init__()\n",
        "\n",
        "        # VGG19\n",
        "        self.vgg = models.vgg19_bn(pretrained=use_pretrained)\n",
        "        self.set_parameter_requires_grad(self.vgg, feature_extract)\n",
        "        num_ftrs_vgg = self.vgg.classifier[6].in_features\n",
        "        self.vgg.classifier[6] = nn.Linear(num_ftrs_vgg, num_classes)\n",
        "\n",
        "        # Vision Transformer\n",
        "        self.vit = models.vit_b_16(pretrained=use_pretrained)\n",
        "        self.set_parameter_requires_grad(self.vit, feature_extract)\n",
        "        num_ftrs_vit = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs_vit, num_classes)\n",
        "\n",
        "        # Combined classifier\n",
        "        self.classifier = nn.Linear(num_classes * 2, num_classes)  # Output from both models\n",
        "\n",
        "    def set_parameter_requires_grad(self, model, feature_extracting):\n",
        "        if feature_extracting:\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through VGG19\n",
        "        vgg_out = self.vgg(x)\n",
        "\n",
        "        # Forward pass through ViT\n",
        "        vit_out = self.vit(x)\n",
        "\n",
        "        # Concatenate outputs from both models\n",
        "        combined_out = torch.cat((vgg_out, vit_out), dim=1)\n",
        "\n",
        "        # Classifier\n",
        "        output = self.classifier(combined_out)\n",
        "        return output\n",
        "\n",
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"hybrid_vgg_vit\":\n",
        "        \"\"\" Hybrid model of VGG19 and ViT \"\"\"\n",
        "        model_ft = HybridModel(num_classes, use_pretrained, feature_extract)\n",
        "        input_size = 224\n",
        "        return model_ft, input_size\n"
      ],
      "metadata": {
        "id": "UTnbq4oEsNAB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"aryanfelix/brats-2019-traintestvalid\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY2GGsAGhjNt",
        "outputId": "d58c6c59-bc8b-4c9e-fbbb-07b4e66e4b82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/aryanfelix/brats-2019-traintestvalid?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 62.9M/62.9M [00:04<00:00, 14.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = \"/root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1\"\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "    print(f\"Root: {root}\")\n",
        "    print(\"Directories:\", dirs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHVeuMslnurl",
        "outputId": "342f4655-6981-48cc-ffe7-265452519ac6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root: /root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1\n",
            "Directories: ['dataset']\n",
            "Root: /root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1/dataset\n",
            "Directories: ['valid', 'train', 'test']\n",
            "Root: /root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1/dataset/valid\n",
            "Directories: ['yes', 'no']\n",
            "Root: /root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1/dataset/valid/yes\n",
            "Directories: []\n",
            "Root: /root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1/dataset/valid/no\n",
            "Directories: []\n",
            "Root: /root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1/dataset/train\n",
            "Directories: ['yes', 'no']\n",
            "Root: /root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1/dataset/train/yes\n",
            "Directories: []\n",
            "Root: /root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1/dataset/train/no\n",
            "Directories: []\n",
            "Root: /root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1/dataset/test\n",
            "Directories: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BeitForImageClassification, BeitFeatureExtractor\n",
        "\n",
        "# Define the hybrid model class\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, num_classes, use_pretrained=True, feature_extract=True):\n",
        "        super(HybridModel, self).__init__()\n",
        "\n",
        "        # VGG19\n",
        "        self.vgg = models.vgg19_bn(pretrained=use_pretrained)\n",
        "        self.set_parameter_requires_grad(self.vgg, feature_extract)\n",
        "        num_ftrs_vgg = self.vgg.classifier[6].in_features\n",
        "        self.vgg.classifier[6] = nn.Linear(num_ftrs_vgg, num_classes)\n",
        "\n",
        "        # Vision Transformer (ViT)\n",
        "        self.vit = models.vit_b_16(pretrained=use_pretrained)\n",
        "        self.set_parameter_requires_grad(self.vit, feature_extract)\n",
        "        num_ftrs_vit = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs_vit, num_classes)\n",
        "\n",
        "        # Combined classifier\n",
        "        self.classifier = nn.Linear(num_classes * 2, num_classes)  # Concatenate outputs from both models\n",
        "\n",
        "    def set_parameter_requires_grad(self, model, feature_extracting):\n",
        "        if feature_extracting:\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through VGG19\n",
        "        vgg_out = self.vgg(x)\n",
        "\n",
        "        # Forward pass through ViT\n",
        "        vit_out = self.vit(x)\n",
        "\n",
        "        # Concatenate outputs from both models\n",
        "        combined_out = torch.cat((vgg_out, vit_out), dim=1)\n",
        "\n",
        "        # Classifier\n",
        "        output = self.classifier(combined_out)\n",
        "        return output\n",
        "\n",
        "# Initialize the model\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"hybrid_vgg_vit\":\n",
        "        model_ft = HybridModel(num_classes, use_pretrained, feature_extract)\n",
        "        input_size = 224\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Custom Dataset class\n",
        "class BraTSDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.file_paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Function to get file paths and labels\n",
        "def get_brats_data_paths(data_dir):\n",
        "    file_paths = []\n",
        "    labels = []\n",
        "\n",
        "    # Define mappings for labels\n",
        "    label_mapping = {'yes': 1, 'no': 0}\n",
        "\n",
        "    # Collect paths and labels from train, valid, and test folders\n",
        "    for split in ['train', 'valid']:\n",
        "        split_dir = os.path.join(data_dir, 'dataset', split)\n",
        "\n",
        "        for label_name in label_mapping.keys():\n",
        "            label_dir = os.path.join(split_dir, label_name)\n",
        "            if os.path.exists(label_dir):\n",
        "                for file_name in os.listdir(label_dir):\n",
        "                    file_paths.append(os.path.join(label_dir, file_name))\n",
        "                    labels.append(label_mapping[label_name])\n",
        "\n",
        "    return file_paths, labels\n",
        "\n",
        "# Set data directory path\n",
        "data_dir = \"/root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1\"\n",
        "\n",
        "# Load data paths and labels\n",
        "file_paths, labels = get_brats_data_paths(data_dir)\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = BraTSDataset(train_paths, train_labels, transform=transform)\n",
        "val_dataset = BraTSDataset(val_paths, val_labels, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "num_classes = 2\n",
        "model_name = \"hybrid_vgg_vit\"\n",
        "feature_extract = True\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Define training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_ft = model_ft.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_ft.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with model saving\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model_ft.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_ft(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track accuracy and loss\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
        "\n",
        "# Save the trained teacher model\n",
        "teacher_model_path = \"hybrid_teacher_model.pth\"\n",
        "torch.save(model_ft.state_dict(), teacher_model_path)\n",
        "print(f\"Teacher model saved at {teacher_model_path}\")\n",
        "\n",
        "# Validation loop\n",
        "model_ft.eval()\n",
        "with torch.no_grad():\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_ft(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = correct / total\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnWQurhDolmr",
        "outputId": "59e5da81-56bb-48c0-dfb2-b48134d36c9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.3614, Accuracy: 0.8600\n",
            "Epoch [2/5], Loss: 0.1676, Accuracy: 0.9483\n",
            "Epoch [3/5], Loss: 0.1144, Accuracy: 0.9654\n",
            "Epoch [4/5], Loss: 0.0891, Accuracy: 0.9733\n",
            "Epoch [5/5], Loss: 0.0641, Accuracy: 0.9821\n",
            "Teacher model saved at hybrid_teacher_model.pth\n",
            "Validation Loss: 0.0518, Validation Accuracy: 0.9867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a02QnYhaovoA",
        "outputId": "f42397b0-e456-4c97-91f8-92ddb4aa70ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (3.0.2)\n",
            "Building wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=c9a1daac82984828da63113461d50d5f1bcc7bdb675efecd299a4eefa1c278af\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: efficientnet_pytorch\n",
            "Successfully installed efficientnet_pytorch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from torch.optim import Adam\n",
        "from transformers import BeitForImageClassification, BeitFeatureExtractor\n",
        "\n",
        "# Define the hybrid model class (Teacher Model)\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, num_classes, use_pretrained=True, feature_extract=True):\n",
        "        super(HybridModel, self).__init__()\n",
        "\n",
        "        # VGG19\n",
        "        self.vgg = models.vgg19_bn(pretrained=use_pretrained)\n",
        "        self.set_parameter_requires_grad(self.vgg, feature_extract)\n",
        "        num_ftrs_vgg = self.vgg.classifier[6].in_features\n",
        "        self.vgg.classifier[6] = nn.Linear(num_ftrs_vgg, num_classes)\n",
        "\n",
        "        # Vision Transformer (ViT)\n",
        "        self.vit = models.vit_b_16(pretrained=use_pretrained)\n",
        "        self.set_parameter_requires_grad(self.vit, feature_extract)\n",
        "        num_ftrs_vit = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs_vit, num_classes)\n",
        "\n",
        "        # Combined classifier\n",
        "        self.classifier = nn.Linear(num_classes * 2, num_classes)  # Concatenate outputs from both models\n",
        "\n",
        "    def set_parameter_requires_grad(self, model, feature_extracting):\n",
        "        if feature_extracting:\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through VGG19\n",
        "        vgg_out = self.vgg(x)\n",
        "\n",
        "        # Forward pass through ViT\n",
        "        vit_out = self.vit(x)\n",
        "\n",
        "        # Concatenate outputs from both models\n",
        "        combined_out = torch.cat((vgg_out, vit_out), dim=1)\n",
        "\n",
        "        # Classifier\n",
        "        output = self.classifier(combined_out)\n",
        "        return output\n",
        "\n",
        "# Initialize the model\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"hybrid_vgg_vit\":\n",
        "        model_ft = HybridModel(num_classes, use_pretrained, feature_extract)\n",
        "        input_size = 224\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Define the student model (EfficientNet)\n",
        "def initialize_student_model(num_classes):\n",
        "    student_model = models.efficientnet_b0(pretrained=True)\n",
        "    num_ftrs = student_model.classifier[1].in_features\n",
        "    student_model.classifier[1] = nn.Linear(num_ftrs, num_classes)  # Adjust for the number of classes\n",
        "    return student_model\n",
        "\n",
        "# Custom Dataset class for BraTS\n",
        "class BraTSDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.file_paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Function to get file paths and labels\n",
        "def get_brats_data_paths(data_dir):\n",
        "    file_paths = []\n",
        "    labels = []\n",
        "\n",
        "    # Define mappings for labels\n",
        "    label_mapping = {'yes': 1, 'no': 0}\n",
        "\n",
        "    # Collect paths and labels from train, valid, and test folders\n",
        "    for split in ['train', 'valid']:\n",
        "        split_dir = os.path.join(data_dir, 'dataset', split)\n",
        "\n",
        "        for label_name in label_mapping.keys():\n",
        "            label_dir = os.path.join(split_dir, label_name)\n",
        "            if os.path.exists(label_dir):\n",
        "                for file_name in os.listdir(label_dir):\n",
        "                    file_paths.append(os.path.join(label_dir, file_name))\n",
        "                    labels.append(label_mapping[label_name])\n",
        "\n",
        "    return file_paths, labels\n",
        "\n",
        "# Set data directory path\n",
        "data_dir = \"/root/.cache/kagglehub/datasets/aryanfelix/brats-2019-traintestvalid/versions/1\"\n",
        "\n",
        "# Load data paths and labels\n",
        "file_paths, labels = get_brats_data_paths(data_dir)\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = BraTSDataset(train_paths, train_labels, transform=transform)\n",
        "val_dataset = BraTSDataset(val_paths, val_labels, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize the teacher model\n",
        "num_classes = 2\n",
        "model_name = \"hybrid_vgg_vit\"\n",
        "feature_extract = True\n",
        "teacher_model, _ = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "teacher_model.to(device)\n",
        "\n",
        "# Load the trained teacher model\n",
        "teacher_model.load_state_dict(torch.load(\"hybrid_teacher_model.pth\"))\n",
        "teacher_model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Initialize the student model (EfficientNet)\n",
        "student_model = initialize_student_model(num_classes)\n",
        "student_model.to(device)\n",
        "\n",
        "# Loss functions\n",
        "criterion_hard = nn.CrossEntropyLoss()\n",
        "criterion_soft = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "\n",
        "# Optimizer for student model\n",
        "optimizer = Adam(student_model.parameters(), lr=0.001)\n",
        "\n",
        "# Hyperparameters for knowledge distillation\n",
        "temperature = 3.0\n",
        "alpha = 0.7  # Balance between hard and soft loss\n",
        "\n",
        "# Training loop for knowledge distillation\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    student_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass through the teacher model (no gradient tracking)\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(images)\n",
        "\n",
        "        # Forward pass through the student model\n",
        "        student_outputs = student_model(images)\n",
        "\n",
        "        # Soft labels from teacher (using temperature)\n",
        "        soft_labels = nn.functional.softmax(teacher_outputs / temperature, dim=1)\n",
        "\n",
        "        # Compute the losses\n",
        "        loss_hard = criterion_hard(student_outputs, labels)\n",
        "        loss_soft = criterion_soft(\n",
        "            nn.functional.log_softmax(student_outputs / temperature, dim=1), soft_labels\n",
        "        )\n",
        "        loss = alpha * loss_soft + (1 - alpha) * loss_hard\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track accuracy and loss\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(student_outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
        "\n",
        "# Save the student model after training\n",
        "student_model_path = \"efficientnet_student_model.pth\"\n",
        "torch.save(student_model.state_dict(), student_model_path)\n",
        "print(f\"Student model saved at {student_model_path}\")\n",
        "\n",
        "# Validation loop for the student model\n",
        "student_model.eval()\n",
        "with torch.no_grad():\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass through the student model\n",
        "        outputs = student_model(images)\n",
        "        loss = criterion_hard(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = correct / total\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4YtpFv1xfx4",
        "outputId": "09286249-0571-4928-d14b-90f0a25f8227"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-964beb3e5868>:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  teacher_model.load_state_dict(torch.load(\"hybrid_teacher_model.pth\"))\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 177MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.0803, Accuracy: 0.9533\n",
            "Epoch [2/5], Loss: 0.0322, Accuracy: 0.9929\n",
            "Epoch [3/5], Loss: 0.0273, Accuracy: 0.9938\n",
            "Epoch [4/5], Loss: 0.0293, Accuracy: 0.9950\n",
            "Epoch [5/5], Loss: 0.0247, Accuracy: 0.9933\n",
            "Student model saved at efficientnet_student_model.pth\n",
            "Validation Loss: 0.0370, Validation Accuracy: 0.9917\n"
          ]
        }
      ]
    }
  ]
}