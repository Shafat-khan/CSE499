In this study we propose a novel approach of Cross-architectural knowledge distillation where we made our own hybrid teacher model with VGG19 (CNN) which captures local features and VIT that captures global features, and we integrated multi-scale geometric feature fusion while applying knowledge-distillation to our lighter student model efficient-net improving for results for Brain tumor classification. We implemented Brain tumor segmentation where we used a U-net encoder decoder + VIT as our teacher model and applied knowledge distillation to a Smaller U-net student model. The research shows improvement in student model performance for classification and segmentation under resource constrained environments.
